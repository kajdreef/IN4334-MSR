\section{Introduction}
\label{sec:introduction}
Software defects correction have a great impact on the economy: it costs tens of billions of dollars every year and the 50\% of developers programming time \cite{DefectsCost, DefectsCost2}. For this reason, in recent years, a wide number of studies focused on defining and comparing software metrics that can be useful to build models for \textit{defect prediction} \cite{bird:original, Rahman:blame, Greiler:replication, Rahman:2013, moser2008comparative, zimmermann2009cross-project}. A \textit{software metric} is a measure of a property of the software that can be related to the code (\textit{code metric}) or to its development process (\textit{process or change metric}). Previous studies also reported that process metrics are better-suited for prediction models \cite{moser2008comparative, Rahman:2013}. These past results showed that the developers behaviour has more impact on software quality than the characteristics of the software itself. \textit{Code ownership} metrics are process metrics, and can in general be defined as measurements of the proportion of contribution of the developers to a source code artifact over a certain period of time, in terms of code changes (e.g. number of commits) \cite{Greiler:replication}.

Previous studies reported contrasting results on the relation between software quality and ownership; this is probably due to the fact that ownership metrics are highly dependant on the process used to develop the software and on the organizational structure of the team. Bird et al.~\cite{bird:original} showed that there is a significant correlation between ownership and number of defects in Microsoft Windows projects, while accordingly to Foucault et al.~\cite{Foucault:oss} the same metrics do not result in the same kind of relationship when computed on open-source software (OSS) artifacts. Foucault et al.~\cite{Foucault:oss} also introduced the idea of computing ownership metrics on artifacts of different granularities (Java files and packages). Greiler et al.~\cite{Greiler:replication} applied the same principle on different Microsoft Products, and their results showed that it can be more significant to use ownership metrics to classify defective and non-defective artifacts rather than to try to infer the number of defects. 

%NOT NEEDED: 
%To obtain more generalizable results ownership metrics should be computed in a way that adapts to the structure of the project.

%Unfortunately, to our knowledge, a classification approach like the one described above has not yet been applied to Open-Source Software. Furthermore, none of the previous studies tried to compute the ownership metrics on the revision of the artifact where the defective code is introduced. In addition to that, all the previously cited works computed the ownership metrics considering as variable the commit count on the software artifacts, and tried to change the granularity of the artifacts themselves, but not the granularity of the considered variable (e.g. considering the amount of lines added and deleted in every commit instead of the commit count).

All the previous studies computed the metrics on a certain release of the software and then tried to find a correlation between these metrics and the number of defects introduced before that release. We think that the metrics should instead be computed on the software artifacts just after the commits that introduce a defect, to better capture the exact state of the code when it becomes defective. In addition to that, most of the previously cited works used the commit count to measure the contribution of the developers to the source code artifacts; none of them tried to compute the ownership metrics in a more fine-grained way (e.g. considering the amount lines of code added and deleted). Furthermore, to our knowledge, a classification approach like the one described in in the previous paragraph has not yet been applied to Open-Source Software.

%Our idea is that in order to build a defect prediction model the ownership metrics should be computed on the releases of software artifacts that succeed the introduction of defective code. 
%To determine when the defective code is introduced we use the concept of \textit{implicated code} as described by Rahman er al.~\cite{Rahman:blame} (also called \textit{fix-inducing} code~\cite{sliwerski2005changes}). 

In this work, we study the effect of code ownership on software quality by trying to classify defective source code files on different Open-Source software projects. We expand the cited previous work by: (1) computing ownership metrics on source files just after some defective code is introduced; (2) experimenting the effects of changing the granularity of the metrics; (3) applying a classification approach to distinguish defective and non-defective source files on Open-Source software projects.

To spot the introduction of defective code we use the concept of \textit{implicated code} as described by Rahman et al.~\cite{Rahman:blame} (also called \textit{fix-inducing} code~\cite{sliwerski2005changes}). 
We selected 5 OSS projects and, for each one of them, we: (1) create  a dataset that captures the history of its development in terms of developers contribution and implicated code introduction (2) use this dataset to create a second dataset that contains the ownership metrics computed for every version of every file, with different granularities (3) use these metrics, together with some classic defect prediction code metrics, to classify the defective file versions with the Random Forests~\cite{breiman2001random} and Logistic Regression techniques (4) measure how the ownership metrics improve the model that uses only the classic ones.

Our results show that ownership metrics are indicative for software defects, giving a significant improvement over a classification model built using only classic code metrics; this is particularly evident when using a more fine-grained, line-based, approach to compute the ownership, as we hypothesized.

We think that our approach can capture the characteristics of the defective code in a better way, leading to more generalizable outcomes.

Code and datasets created for this work are publicly available.\footnote{\url{github.com/kajdreef/IN4334-MSR}}

