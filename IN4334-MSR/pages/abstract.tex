\begin{abstract}
Code ownership measures the proportion of contribution of the developers to a source code artifact, in terms of code changes (e.g. number of commits). It can be used to describe the responsibility for a certain piece of software or the expertise of the developers with respect to it. Responsibility and expertise are important factors in the development process, and can affect software quality: for this reason a lot of studies focused on determining how to measure the ownership in order to include it in defect prediction models in an effective way. Bird et al~\cite{bird:original} and Greiler et al.~\cite{Greiler:replication} showed that ownership metrics are a good indicator for software quality in Microsoft software projects, while Foucault et al.~\cite{Foucault:oss} found contrasting results for what concerns open-source software projects. However, in our opinion these past studies compute the ownership without considering the software revisions that actually introduce defects. Furthermore, no study did an explicit analysis of the effect of the granularity chosen to consider the code changes (e.g. considering lines instead of commits).

In this paper we contribute to the past research in the following ways: (1) we build dataset, publicly available, that contains information about code changes and defects over the whole development history for five open-source software projects; (2) we describe and apply a novel technique to compute software metrics, using the above mentioned dataset, that can capture the state of the software right after the introduction of defective code; (3) we use this technique to empirically study the effect that ownership has on software quality for five open-source software projects, considering an exhaustive set of metrics in terms of granularity of code changes.

Using this approach and considering also a set of classic and effective code metrics we are able to classify defective files, using Random Forest, with an average out-of-bag error rate of 23\% and an average relative improvement of 22\% over a model that uses only the classic metrics. %Also line based metrics have on average a performance increase of 17\% in comparison to commit based metrics.
\end{abstract}